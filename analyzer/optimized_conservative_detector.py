"""
Optimized Conservative Evasion-Resistant AI Detection Engine
High-performance version optimized for large file sets with caching and parallel processing
"""

import re
import json
import hashlib
from typing import Dict, List, Any, Set, Tuple, Optional
from collections import Counter
from functools import lru_cache
import threading

class OptimizedConservativeDetector:
    def __init__(self, indicators_config: Dict[str, Any]):
        self.config = indicators_config
        self._compile_patterns()
        self._initialize_weights()
        self._initialize_thresholds()
        self._pattern_cache = {}
        self._cache_lock = threading.Lock()
    
    def _compile_patterns(self):
        """Pre-compile all regex patterns for performance"""
        
        # Explicit AI markers (compiled once)
        self.compiled_explicit_markers = [
            re.compile(pattern, re.IGNORECASE | re.MULTILINE)
            for pattern in [
                r'#\s*(?:Generated by|Created by|Code by)\s*(?:Copilot|AI|GPT|Claude)',
                r'//\s*(?:Generated by|Created by|Code by)\s*(?:Copilot|AI|GPT|Claude)',
                r'/\*\s*(?:Generated by|Created by|Code by)\s*(?:Copilot|AI|GPT|Claude)',
                r'#\s*AI-generated',
                r'//\s*AI-generated',
            ]
        ]
        
        # Strong semantic patterns (compiled)
        self.compiled_semantic_patterns = [
            re.compile(pattern, re.MULTILINE | re.DOTALL)
            for pattern in [
                # Python patterns
                r'def\s+\w+\([^)]*\)\s*->\s*(?:Optional\[.*\]|Union\[.*\]|Dict\[.*\]):\s*\n\s*"""[\s\S]{50,200}?Args:[\s\S]{20,100}?Returns:',
                r'try:\s*\n[^}]{10,100}except\s+Exception\s+as\s+e:\s*\n\s*(?:print\(|logger\.|pass\s*$)',
                # Java patterns
                r'public\s+(?:static\s+)?(?:void|\w+)\s+\w+\([^)]*\)\s+throws\s+Exception\s*\{[\s\S]{50,300}?catch\s*\(\s*Exception\s+e\s*\)\s*\{[\s\S]{10,100}?e\.printStackTrace\(\)',
                r'@Override\s*\n\s*public\s+(?:boolean\s+equals|int\s+hashCode|String\s+toString)',
            ]
        ]
        
        # Distinctive naming patterns (compiled)
        self.compiled_naming_patterns = [
            re.compile(pattern)
            for pattern in [
                # Python style
                r'\b(?:data|result|response|output|input)_\d+\b',
                r'\b(?:temp|tmp)_(?:data|result|value|obj)\b',
                r'\b(?:process|handle|execute)_(?:data|input|request)_\w+\b',
                # Java style
                r'\b(?:userData|responseData|requestData|inputData)\d*\b',
                r'\b(?:tempObject|tempValue|tempResult)\d*\b',
                r'\b(?:processUserData|handleUserRequest|executeUserAction)\b',
            ]
        ]
        
        # Simple evasion detection patterns
        self.compiled_evasion_patterns = {
            'systematic_renaming': re.compile(r'\b[a-z]\d*\b'),
            'numbered_vars': re.compile(r'\b\w+\d+\b'),
            'single_letters': re.compile(r'\b[a-z]\b'),
        }
    
    def _initialize_weights(self):
        """Initialize optimized weights"""
        self.weights = {
            'explicit_markers': 0.50,     # Higher weight for explicit detection
            'strong_semantics': 0.25,     # Reduced complexity
            'distinctive_naming': 0.15,   # Simplified naming check
            'evasion_resistance': 0.10,   # Lightweight evasion detection
        }
    
    def _initialize_thresholds(self):
        """Initialize conservative thresholds"""
        self.thresholds = {
            'high_confidence': 0.80,
            'medium_confidence': 0.50,
            'low_confidence': 0.25,
            'minimal_threshold': 0.10,
        }
        
        # Optimized calibration
        self.calibration = {
            'base_reduction': 0.65,
            'language_factors': {
                'python': 1.0,
                'javascript': 0.8,
                'java': 0.85,
                'typescript': 0.85,
            }
        }
    
    def analyze_content(self, content: str, file_extension: str) -> Dict[str, Any]:
        """Optimized analysis with caching"""
        
        if not content.strip():
            return self._create_empty_analysis()
        
        # Create content hash for caching
        content_hash = hashlib.md5(content.encode()).hexdigest()
        cache_key = f"{content_hash}_{file_extension}"
        
        # Check cache first
        with self._cache_lock:
            if cache_key in self._pattern_cache:
                return self._pattern_cache[cache_key].copy()
        
        # Fast early exit for very small files
        if len(content) < 100:
            return self._create_minimal_analysis()
        
        # Optimized detection
        explicit_score = self._fast_explicit_detection(content)
        semantic_score = self._fast_semantic_detection(content)
        naming_score = self._fast_naming_detection(content)
        evasion_score = self._fast_evasion_detection(content)
        
        # Calculate confidence
        raw_confidence = (
            explicit_score * self.weights['explicit_markers'] +
            semantic_score * self.weights['strong_semantics'] +
            naming_score * self.weights['distinctive_naming'] +
            evasion_score * self.weights['evasion_resistance']
        )
        
        # Apply calibration
        calibrated_confidence = self._apply_fast_calibration(raw_confidence, content, file_extension)
        
        # Create result
        result = {
            'copilot_confidence': calibrated_confidence,
            'confidence_score': calibrated_confidence,
            'estimated_lines': self._estimate_lines(calibrated_confidence, content),
            'risk_level': self._calculate_risk_level(calibrated_confidence),
            'language': self._detect_language(file_extension),
            'conservative_analysis': {
                'explicit_score': explicit_score,
                'semantic_score': semantic_score,
                'naming_score': naming_score,
                'evasion_score': evasion_score,
                'raw_confidence': raw_confidence,
                'calibration_applied': raw_confidence - calibrated_confidence
            },
            'evasion_resistance': {
                'evasion_detected': evasion_score > 0,
                'evasion_indicators': self._get_evasion_indicators(evasion_score),
                'maintains_detection': calibrated_confidence > 0.10
            },
            'explanation': self._generate_fast_explanation(calibrated_confidence, explicit_score)
        }
        
        # Cache result
        with self._cache_lock:
            self._pattern_cache[cache_key] = result.copy()
            # Limit cache size
            if len(self._pattern_cache) > 1000:
                # Remove oldest 200 entries
                keys_to_remove = list(self._pattern_cache.keys())[:200]
                for key in keys_to_remove:
                    del self._pattern_cache[key]
        
        return result
    
    def _fast_explicit_detection(self, content: str) -> float:
        """Fast explicit marker detection"""
        for pattern in self.compiled_explicit_markers:
            if pattern.search(content):
                return 1.0
        return 0.0
    
    def _fast_semantic_detection(self, content: str) -> float:
        """Fast semantic pattern detection"""
        matches = 0
        total_patterns = len(self.compiled_semantic_patterns)
        
        for pattern in self.compiled_semantic_patterns:
            if pattern.search(content):
                matches += 1
                if matches >= 2:  # Early exit optimization
                    break
        
        return min(matches / total_patterns * 1.5, 1.0)
    
    def _fast_naming_detection(self, content: str) -> float:
        """Fast naming pattern detection"""
        # Quick estimation using simple counts
        total_words = len(re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', content[:2000]))  # Sample first 2000 chars
        if total_words < 10:
            return 0.0
        
        ai_names = 0
        for pattern in self.compiled_naming_patterns:
            matches = pattern.findall(content[:2000])  # Sample only
            ai_names += len(matches)
        
        ratio = ai_names / total_words
        return min(ratio * 3, 1.0) if ratio > 0.05 else 0.0
    
    def _fast_evasion_detection(self, content: str) -> float:
        """Fast evasion detection"""
        lines = content.split('\n')
        if len(lines) < 20:
            return 0.0
        
        # Sample lines for performance
        sample_lines = lines[::max(1, len(lines)//50)][:50]
        sample_content = '\n'.join(sample_lines)
        
        evasion_score = 0.0
        
        # Check for systematic renaming
        single_letter_vars = len(self.compiled_evasion_patterns['single_letters'].findall(sample_content))
        total_vars = len(re.findall(r'\b[a-zA-Z_]\w*\b', sample_content))
        
        if total_vars > 10 and single_letter_vars / total_vars > 0.3:
            evasion_score += 0.5
        
        return min(evasion_score, 1.0)
    
    def _apply_fast_calibration(self, raw_confidence: float, content: str, file_extension: str) -> float:
        """Fast calibration"""
        # Simple calibration
        calibrated = raw_confidence * self.calibration['base_reduction']
        
        # Language factor
        language = self._detect_language(file_extension)
        lang_factor = self.calibration['language_factors'].get(language, 0.8)
        calibrated *= lang_factor
        
        # File size factor (simple)
        if len(content) > 5000:
            calibrated *= 0.9
        
        return min(calibrated, 1.0)
    
    def _estimate_lines(self, confidence: float, content: str) -> int:
        """Fast line estimation"""
        total_lines = content.count('\n') + 1
        code_lines = max(1, total_lines - content.count('\n#') - content.count('\n//'))
        
        if confidence >= 0.6:
            return int(code_lines * confidence * 0.7)
        elif confidence >= 0.3:
            return int(code_lines * confidence * 0.5)
        else:
            return int(code_lines * confidence * 0.3)
    
    def _calculate_risk_level(self, confidence: float) -> str:
        """Fast risk calculation"""
        if confidence >= self.thresholds['high_confidence']:
            return "HIGH"
        elif confidence >= self.thresholds['medium_confidence']:
            return "MEDIUM"
        elif confidence >= self.thresholds['low_confidence']:
            return "LOW"
        else:
            return "MINIMAL"
    
    def _get_evasion_indicators(self, evasion_score: float) -> List[str]:
        """Fast evasion indicator generation"""
        indicators = []
        if evasion_score > 0.3:
            indicators.append('systematic_variable_renaming')
        if evasion_score > 0.5:
            indicators.append('potential_obfuscation')
        return indicators
    
    def _generate_fast_explanation(self, confidence: float, explicit_score: float) -> List[str]:
        """Fast explanation generation"""
        explanation = []
        
        if explicit_score > 0:
            explanation.append("Explicit AI markers detected")
        elif confidence >= 0.4:
            explanation.append("Strong AI patterns detected")
        elif confidence >= 0.2:
            explanation.append("Moderate AI patterns detected")
        elif confidence >= 0.1:
            explanation.append("Minimal AI patterns detected")
        else:
            explanation.append("No significant AI patterns detected")
        
        return explanation
    
    def _detect_language(self, file_extension: str) -> str:
        """Fast language detection"""
        language_map = {
            '.py': 'python', '.js': 'javascript', '.ts': 'typescript',
            '.java': 'java', '.cpp': 'cpp', '.c': 'c', '.cs': 'csharp',
            '.go': 'go', '.rs': 'rust', '.php': 'php', '.rb': 'ruby'
        }
        return language_map.get(file_extension.lower(), 'unknown')
    
    def _create_empty_analysis(self) -> Dict[str, Any]:
        """Create empty analysis for invalid content"""
        return {
            'copilot_confidence': 0.0,
            'confidence_score': 0.0,
            'estimated_lines': 0,
            'risk_level': 'MINIMAL',
            'language': 'unknown',
            'conservative_analysis': {
                'explicit_score': 0.0, 'semantic_score': 0.0,
                'naming_score': 0.0, 'evasion_score': 0.0,
                'raw_confidence': 0.0, 'calibration_applied': 0.0
            },
            'evasion_resistance': {
                'evasion_detected': False, 'evasion_indicators': [],
                'maintains_detection': False
            },
            'explanation': ['Empty or invalid content']
        }
    
    def _create_minimal_analysis(self) -> Dict[str, Any]:
        """Create minimal analysis for very small files"""
        return {
            'copilot_confidence': 0.0,
            'confidence_score': 0.0,
            'estimated_lines': 0,
            'risk_level': 'MINIMAL',
            'language': 'unknown',
            'conservative_analysis': {
                'explicit_score': 0.0, 'semantic_score': 0.0,
                'naming_score': 0.0, 'evasion_score': 0.0,
                'raw_confidence': 0.0, 'calibration_applied': 0.0
            },
            'evasion_resistance': {
                'evasion_detected': False, 'evasion_indicators': [],
                'maintains_detection': False
            },
            'explanation': ['File too small for analysis']
        }
    
    def clear_cache(self):
        """Clear pattern cache to free memory"""
        with self._cache_lock:
            self._pattern_cache.clear()
    
    def get_cache_stats(self) -> Dict[str, int]:
        """Get cache statistics"""
        with self._cache_lock:
            return {
                'cache_size': len(self._pattern_cache),
                'cache_limit': 1000
            }